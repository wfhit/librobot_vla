name: Self-Hosted Tests

# This workflow runs tests on self-hosted runners
# It is useful for testing on GPU hardware or custom robot environments
#
# To use this workflow:
# 1. Set up a self-hosted runner (see docs/self_hosted_runner.md)
# 2. Ensure the runner has the appropriate labels (self-hosted, linux, gpu)
# 3. The workflow will automatically run when changes are pushed

on:
  push:
    branches:
      - main
      - develop
  pull_request:
    branches:
      - main
      - develop
  workflow_dispatch:  # Allows manual triggering from GitHub UI
    inputs:
      run_gpu_tests:
        description: 'Run GPU-specific tests'
        required: false
        default: 'true'
        type: boolean

jobs:
  # Basic tests on self-hosted runner
  self-hosted-test:
    name: Self-Hosted Tests
    # Specify runner labels - adjust based on your runner configuration
    # Common labels: self-hosted, linux, windows, macos, gpu, cuda
    runs-on: [self-hosted, linux]
    timeout-minutes: 60  # Prevent runaway jobs
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          clean: true  # Clean checkout to avoid stale files

      - name: Show system information
        run: |
          echo "=== System Information ==="
          uname -a
          echo ""
          echo "=== CPU Information ==="
          lscpu | head -20
          echo ""
          echo "=== Memory Information ==="
          free -h
          echo ""
          echo "=== Disk Space ==="
          df -h .
          echo ""
          echo "=== Python Version ==="
          python3 --version

      - name: Set up Python environment
        run: |
          # Create virtual environment (recommended for self-hosted runners)
          python3 -m venv venv
          source venv/bin/activate
          pip install --upgrade pip
          pip install -e ".[dev]"

      - name: Run unit tests
        run: |
          source venv/bin/activate
          pytest tests/unit/ -v --cov=librobot --cov-report=term

      - name: Run integration tests
        run: |
          source venv/bin/activate
          pytest tests/integration/ -v

      - name: Cleanup
        if: always()
        run: |
          # Clean up virtual environment to save disk space
          rm -rf venv
          # Clean up any cache files
          rm -rf .pytest_cache
          rm -rf .coverage
          rm -rf htmlcov

  # GPU-specific tests (only runs on runners with GPU label)
  gpu-tests:
    name: GPU Tests
    runs-on: [self-hosted, linux, gpu]
    timeout-minutes: 120
    # Only run if manually triggered with gpu tests enabled, or on main branch push
    if: |
      (github.event_name == 'workflow_dispatch' && github.event.inputs.run_gpu_tests == 'true') ||
      (github.event_name == 'push' && github.ref == 'refs/heads/main')
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          clean: true

      - name: Check GPU availability
        run: |
          echo "=== GPU Information ==="
          nvidia-smi || echo "nvidia-smi not available"
          echo ""
          echo "=== CUDA Version ==="
          nvcc --version || echo "CUDA not installed"

      - name: Set up Python environment
        run: |
          python3 -m venv venv
          source venv/bin/activate
          pip install --upgrade pip
          pip install -e ".[dev]"
          # Install GPU-specific dependencies
          pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118

      - name: Run GPU tests
        run: |
          source venv/bin/activate
          # Run tests marked with gpu marker
          pytest tests/ -v -m gpu --timeout=300 || true

      - name: Run VLA inference test
        run: |
          source venv/bin/activate
          # Test VLA model inference if available
          python -c "
          import torch
          print(f'PyTorch version: {torch.__version__}')
          print(f'CUDA available: {torch.cuda.is_available()}')
          if torch.cuda.is_available():
              print(f'GPU: {torch.cuda.get_device_name(0)}')
              print(f'GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')
          "

      - name: Cleanup
        if: always()
        run: |
          rm -rf venv
          rm -rf .pytest_cache
          rm -rf .coverage

  # Robot hardware tests (for runners connected to physical robots)
  robot-hardware-tests:
    name: Robot Hardware Tests
    runs-on: [self-hosted, robot-hardware]
    timeout-minutes: 30
    # Only run on manual trigger - hardware tests should be explicitly requested
    if: github.event_name == 'workflow_dispatch'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python environment
        run: |
          python3 -m venv venv
          source venv/bin/activate
          pip install --upgrade pip
          pip install -e ".[dev]"

      - name: Check robot connection
        run: |
          source venv/bin/activate
          # Add robot-specific connection check here
          echo "Checking robot hardware connection..."
          # python -c "from librobot.robots import check_connection; check_connection()"

      - name: Run hardware tests
        run: |
          source venv/bin/activate
          # Run tests marked with hardware marker
          pytest tests/ -v -m hardware --timeout=60 || true

      - name: Cleanup
        if: always()
        run: |
          rm -rf venv
