# Standard training configuration

batch_size: 4
num_workers: 4
max_steps: 100000
max_epochs: null
gradient_accumulation_steps: 1
gradient_clip_norm: 1.0
save_interval: 5000
eval_interval: 1000

optimizer:
  name: adamw
  lr: 1e-4
  weight_decay: 0.01
  betas: [0.9, 0.999]

scheduler:
  name: cosine
  warmup_steps: 1000
  min_lr: 1e-6

# DeepSpeed configuration (optional)
use_deepspeed: false
deepspeed_config: null
